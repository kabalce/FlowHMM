@inproceedings{
lorek2022flowhmm,
title={Flow{HMM}: Flow-based continuous hidden Markov models},
author={Pawel Lorek and Rafa{\l} Nowak and Tomasz Trzcinski and Maciej Zieba},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=hFa75frAh0}
}

@misc{ghosh_normalizing_2021,
	title = {Normalizing {Flow} based {Hidden} {Markov} {Models} for {Classification} of {Speech} {Phones} with {Explainability}},
	url = {http://arxiv.org/abs/2107.00730},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Ghosh, Anubhab and Honoré, Antoine and Liu, Dong and Henter, Gustav Eje and Chatterjee, Saikat},
	month = jul,
	year = {2021},
	note = {arXiv:2107.00730 [cs, eess]},
	annote = {Comment: 12 pages, 4 figures},
	file = {Ghosh et al. - 2021 - Normalizing Flow based Hidden Markov Models for Cl.pdf:/home/kabalce/Zotero/storage/ARQVFP3S/Ghosh et al. - 2021 - Normalizing Flow based Hidden Markov Models for Cl.pdf:application/pdf},
}

@inproceedings{inproceedings,
author = {Lakshminarayanan, Balaji and Raich, Raviv},
year = {2010},
month = {10},
pages = {89 - 94},
title = {Non-negative matrix factorization for parameter estimation in hidden Markov models},
doi = {10.1109/MLSP.2010.5589231}
}

@article{mor_systematic_2021,
	title = {A {Systematic} {Review} of {Hidden} {Markov} {Models} and {Their} {Applications}},
	volume = {28},
	issn = {1886-1784},
	url = {https://doi.org/10.1007/s11831-020-09422-4},
	doi = {10.1007/s11831-020-09422-4},
	abstract = {The hidden Markov models are statistical models used in many real-world applications and communities. The use of hidden Markov models has become predominant in the last decades, as evidenced by a large number of published papers. In this survey, 146 papers (101 from Journals and 45 from Conferences/Workshops) from 93 Journals and 44 Conferences/Workshops are considered. The authors evaluate the literature based on hidden Markov model variants that have been applied to various application fields. The paper represents a short but comprehensive description of research on hidden Markov model and its variants for various applications. The paper shows the significant trends in the research on hidden Markov model variants and their applications.},
	number = {3},
	journal = {Archives of Computational Methods in Engineering},
	author = {Mor, Bhavya and Garhwal, Sunita and Kumar, Ajay},
	month = may,
	year = {2021},
	pages = {1429--1448},
}

@article{deshmukh_comparison_2020,
	title = {Comparison of {Hidden} {Markov} {Model} and {Recurrent} {Neural} {Network} in {Automatic} {Speech} {Recognition}},
	volume = {5},
	copyright = {Copyright (c) 2020 Akshay Madhav Deshmukh},
	issn = {2736-576X},
	url = {https://www.ej-eng.org/index.php/ejeng/article/view/2077},
	doi = {10.24018/ejeng.2020.5.8.2077},
	abstract = {Understanding human speech precisely by a machine has been a major challenge for many years.With Automatic Speech Recognition (ASR) being decades old and considering the advancement of the technology, where it is not at the point where machines understand all speech, it is used on a regular basis in many applications and services. Hence, to advance research it is important to identify significant research directions, specifically to those that have not been pursued or funded in the past. The performance of such ASR systems, traditionally build upon an Hidden Markov Model (HMM), has improved due tothe application of Deep Neural Networks (DNNs). Despite this progress, building an ASR system remained a challenging task requiring multiple resources and training stages. The idea of using DNNs for Automatic Speech Recognition has gone further from being a single component in a pipeline to building a system mainly based on such a network.This paper provides a literature survey on state of the art researches on two major models, namely Deep Neural Network - Hidden Markov Model (DNN-HMM) and Recurrent Neural Networks trained with Connectionist Temporal Classification (RNN-CTC). It also provides the differences between these two models at the architectural level.},
	language = {en},
	number = {8},
	urldate = {2023-05-07},
	journal = {European Journal of Engineering and Technology Research},
	author = {Deshmukh, Akshay Madhav},
	month = aug,
	year = {2020},
	note = {Number: 8},
	keywords = {Gaussian Mixture Model},
	pages = {958--965},
	file = {Full Text PDF:/home/kabalce/Zotero/storage/7JQXXIUZ/Deshmukh - 2020 - Comparison of Hidden Markov Model and Recurrent Ne.pdf:application/pdf},
}

@article{zhang_high-order_2019,
	title = {High-order {Hidden} {Markov} {Model} for trend prediction in financial time series},
	volume = {517},
	issn = {0378-4371},
	url = {https://www.sciencedirect.com/science/article/pii/S0378437118314018},
	doi = {10.1016/j.physa.2018.10.053},
	abstract = {Financial price series trend prediction is an essential problem which has been discussed extensively using tools and techniques of economic physics and machine learning. Time dependence and volatility issues in this problem have made Hidden Markov Model (HMM) a useful tool in predicting the states of stock market. In this paper, we present an approach to predict the stock market price trend based on high-order HMM. Different from the commonly used first-order HMM, short and long-term time dependence are both considered in the high order HMM. By introducing a dimension reduction method which could transform the high-dimensional state vector of high-order HMM into a single one, we present a dynamic high-order HMM trading strategy to predict and trade CSI 300 and S\&P 500 stock index for the next day given historical data. In our approach, we make a statistic of the daily returns in the history to demonstrate the relationship between hidden states and the price change trend. Experiments on CSI 300 and S\&P 500 index illustrate that high-order HMM has preferable ability to identify market price trend than first-order one. Thus, the high-order HMM has higher accuracy and lower risk than the first-order model in predicting the index price trend.},
	language = {en},
	urldate = {2023-05-07},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Zhang, Mengqi and Jiang, Xin and Fang, Zehua and Zeng, Yue and Xu, Ke},
	month = mar,
	year = {2019},
	keywords = {High-order HMM, Trading algorithm, Trend prediction},
	pages = {1--12},
	file = {ScienceDirect Full Text PDF:/home/kabalce/Zotero/storage/7YZZ3QBI/Zhang et al. - 2019 - High-order Hidden Markov Model for trend predictio.pdf:application/pdf;ScienceDirect Snapshot:/home/kabalce/Zotero/storage/Z5A8I3GS/S0378437118314018.html:text/html},
}

@incollection{siu_hidden_2014,
	address = {Boston, MA},
	title = {A {Hidden} {Markov}-{Modulated} {Jump} {Diffusion} {Model} for {European} {Option} {Pricing}},
	isbn = {978-1-4899-7442-6},
	url = {https://doi.org/10.1007/978-1-4899-7442-6_8},
	abstract = {The valuation of a European-style contingent claim is discussed in a hidden Markov regime-switching jump-diffusion market, where the evolution of a hidden economic state process over time is described by a continuous-time, finite-state, hidden Markov chain. A two-stage procedure is used to discuss the option valuation problem. Firstly filtering theory is employed to transform the original market with hidden quantities into a filtered market with complete observations. Then a generalized version of the Esscher transform based on a Doléan-Dade stochastic exponential is employed to select a pricing kernel in the filtered market. A partial-differential-integral equation for the price of a European-style option is presented.},
	booktitle = {Hidden {Markov} {Models} in {Finance}: {Further} {Developments} and {Applications}, {Volume} {II}},
	publisher = {Springer US},
	author = {Siu, Tak Kuen},
	editor = {Mamon, Rogemar S. and Elliott, Robert J.},
	year = {2014},
	doi = {10.1007/978-1-4899-7442-6_8},
	pages = {185--209},
}

@article{krogh_hidden_1994,
	title = {Hidden {Markov} {Models} in {Computational} {Biology}: {Applications} to {Protein} {Modeling}},
	volume = {235},
	issn = {0022-2836},
	shorttitle = {Hidden {Markov} {Models} in {Computational} {Biology}},
	url = {https://www.sciencedirect.com/science/article/pii/S0022283684711041},
	doi = {10.1006/jmbi.1994.1104},
	abstract = {Hidden Markov Models (HMMs) are applied to the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated on the globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the SWISS-PROT 22 database for other sequences that are members of the given protein family, or contain the given domain. The HMM produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate three-dimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appears to have a slight advantage over PROFILESEARCH in terms of lower rates of false negatives and false positives, even though the HMM is trained using only unaligned sequences, whereas PROFILESEARCH requires aligned training sequences. Our results suggest the presence of an EF-hand calcium binding motif in a highly conserved and evolutionary preserved putative intracellular region of 155 residues in the α-1 subunit of L-type calcium channels which play an important role in excitation-contraction coupling. This region has been suggested to contain the functional domains that are typical or essential for all L-type calcium channels regardless of whether they couple to ryanodine receptors, conduct ions or both.},
	language = {en},
	number = {5},
	urldate = {2023-05-07},
	journal = {Journal of Molecular Biology},
	author = {Krogh, Anders and Brown, Michael and Mian, I. Saira and Sjölander, Kimmen and Haussler, David},
	month = feb,
	year = {1994},
	keywords = {EF-hand, globin, hidden Markov models, kinase, multiple sequence alignments},
	pages = {1501--1531},
	file = {ScienceDirect Snapshot:/home/kabalce/Zotero/storage/AS6MHTZD/S0022283684711041.html:text/html},
}

@article{momenzadeh_using_2020,
	title = {Using hidden {Markov} model to predict recurrence of breast cancer based on sequential patterns in gene expression profiles},
	volume = {111},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420301969},
	doi = {10.1016/j.jbi.2020.103570},
	abstract = {A new approach is presented to predict breast cancer recurrence through gene expression profiles using hidden Markov models (HMM). In this regard, 322 genes were selected from 44 published gene lists related to breast cancer prognosis. Afterwards, using gene set enrichment analysis, 922 gene sets were found from subsets of genes with the same biological meaning. In order to extract the sequential patterns from gene expression data, we ranked the gene sets using appropriate criteria and used HMM in which the ranked gene sets considered as observation sequences and hidden states represented priority of gene sets for discriminating between expression profiles. In this experiment, seven publicly available microarray datasets, including 1271 breast tumor samples, were used to classify cancer patients into two groups according to risk of recurrence. Our experiments indicated the greater performance and more robustness of the proposed model compared with other widely used classification methods.},
	language = {en},
	urldate = {2023-05-07},
	journal = {Journal of Biomedical Informatics},
	author = {Momenzadeh, Mohammadreza and Sehhati, Mohammadreza and Rabbani, Hossein},
	month = nov,
	year = {2020},
	keywords = {Breast cancer recurrence, Classification, DNA microarray, Gene set enrichment, Hidden Markov model (HMM)},
	pages = {103570},
	file = {ScienceDirect Full Text PDF:/home/kabalce/Zotero/storage/S6KSUU4Q/Momenzadeh et al. - 2020 - Using hidden Markov model to predict recurrence of.pdf:application/pdf;ScienceDirect Snapshot:/home/kabalce/Zotero/storage/625HIFXC/S1532046420301969.html:text/html},
}

@article{juang_hidden_1991,
	title = {Hidden {Markov} {Models} for {Speech} {Recognition}},
	volume = {33},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1991.10484833},
	doi = {10.1080/00401706.1991.10484833},
	abstract = {The use of hidden Markov models for speech recognition has become predominant in the last several years, as evidenced by the number of published papers and talks at major speech conferences. The reasons this method has become so popular are the inherent statistical (mathematically precise) framework; the ease and availability of training algorithms for cstimating the parameters of the models from finite training sets of speech data; the flexibility of the resulting recognition system in which one can easily change the size, type, or architecture of the models to suit particular words, sounds, and so forth; and the ease of implementation of the overall recognition system. In this expository article, we address the role of statistical methods in this powerful technology as applied to speech recognition and discuss a range of theoretical and practical issues that are as yet unsolved in terms of their importance and their effect on performance for different system implementations.},
	number = {3},
	urldate = {2023-05-07},
	journal = {Technometrics},
	author = {Juang, B. H. and Rabiner, L. R.},
	month = aug,
	year = {1991},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1991.10484833},
	keywords = {Baum–Welch algorithm, Incomplete data problem, Maximum a posteriori decoding, Maximum likelihood},
	pages = {251--272},
}

@article{picone_continuous_1990,
	title = {Continuous speech recognition using hidden {Markov} models},
	volume = {7},
	issn = {1558-1284},
	doi = {10.1109/53.54527},
	abstract = {The use of hidden Markov models (HMMs) in continuous speech recognition is reviewed. Markov models are presented as a generalization of their predecessor technology, dynamic programming. A unified view is offered in which both linguistic decoding and acoustic matching are integrated into a single, optimal network search framework. Advances in recognition architectures are discussed. The fundamentals of Viterbi beam search, the dominant search algorithm used today in speed recognition, are presented. Approaches to estimating the probabilities associated with an HMM model are examined. The HMM-supervised training paradigm is examined. Several examples of successful HMM-based speech recognition systems are reviewed.{\textless}{\textgreater}},
	number = {3},
	journal = {IEEE ASSP Magazine},
	author = {Picone, J.},
	month = jul,
	year = {1990},
	note = {Conference Name: IEEE ASSP Magazine},
	keywords = {Acoustic applications, Acoustic signal processing, Decoding, Dynamic programming, Hidden Markov models, Mathematical model, Natural languages, Signal processing, Speech processing, Speech recognition},
	pages = {26--41},
	file = {IEEE Xplore Abstract Record:/home/kabalce/Zotero/storage/5IBMRE29/54527.html:text/html},
}

@inproceedings{murphy_hidden_nodate,
  title={Hidden semi-Markov models ( HSMMs )},
  author={Kevin P. Murphy},
  year={2002},
  url={https://api.semanticscholar.org/CorpusID:18374858},
booktitle = {}
}

@article{dias_heterogeneous_nodate,
author = {Dias, José and Vermunt, Jeroen and Ramos, Sofia},
year = {2008},
month = {01},
pages = {},
title = {Heterogeneous hidden Markov models},
journal = {COMPSTAT2008. Proceedings in Computational Statistics}
}

@inproceedings{thede_second-order_1999,
author = {Thede, Scott M. and Harper, Mary P.},
title = {A Second-Order Hidden Markov Model for Part-of-Speech Tagging},
year = {1999},
isbn = {1558606093},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1034678.1034712},
doi = {10.3115/1034678.1034712},
abstract = {This paper describes an extension to the hidden Markov model for part-of-speech tagging using second-order approximations for both contextual and lexical probabilities. This model increases the accuracy of the tagger to state of the art levels. These approximations make use of more contextual information than standard statistical systems. New methods of smoothing the estimated probabilities are also introduced to address the sparse data problem.},
booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics},
pages = {175–182},
numpages = {8},
location = {College Park, Maryland},
series = {ACL '99}
}

@article{du_preez_efficient_1998,
	title = {Efficient training of high-order hidden {Markov} models using first-order representations},
	volume = {12},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230897900371},
	doi = {https://doi.org/10.1006/csla.1997.0037},
	abstract = {We detail an algorithm (ORED) that transforms any higher-order hidden Markov model (HMM) to an equivalent first-order HMM. This makes it possible to process higher-order HMMs with standard techniques applicable to first-order models. Based on this equivalence, a fast incremental algorithm (FIT) is developed for training higher-order HMMs from lower-order models, thereby avoiding the training of redundant parameters. We also show that the FIT algorithm results in much faster training and better generalization compared to conventional high-order HMM approaches. This makes training of high-order HMMs practical for many applications.},
	number = {1},
	journal = {Computer Speech \& Language},
	author = {Preez, J. A. du},
	year = {1998},
	pages = {23--39},
}

@inproceedings{sicking_densehmm_2022,
	address = {Online Streaming, --- Select a Country ---},
	title = {{DenseHMM}: {Learning} {Hidden} {Markov} {Models} by {Learning} {Dense} {Representations}:},
	isbn = {978-989-758-549-4},
	shorttitle = {{DenseHMM}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010821800003122},
	doi = {10.5220/0010821800003122},
	language = {en},
	urldate = {2023-07-29},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Pattern} {Recognition} {Applications} and {Methods}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Sicking, Joachim and Pintz, Maximilian and Akila, Maram and Wirtz, Tim},
	year = {2022},
	pages = {237--248},
	file = {Sicking et al. - 2022 - DenseHMM Learning Hidden Markov Models by Learnin.pdf:/home/kabalce/Zotero/storage/L34J2EXP/Sicking et al. - 2022 - DenseHMM Learning Hidden Markov Models by Learnin.pdf:application/pdf},
}


@INPROCEEDINGS{lakshminarayanan_non-negative_2010-1,

  author={Lakshminarayanan, Balaji and Raich, Raviv},

  booktitle={2010 IEEE International Workshop on Machine Learning for Signal Processing}, 

  title={Non-negative matrix factorization for parameter estimation in hidden Markov models}, 

  year={2010},

  volume={},

  number={},

  pages={89-94},

  doi={10.1109/MLSP.2010.5589231}}


@article{krakovna_increasing_2016,
author = {Krakovna, Viktoriya and Doshi-Velez, Finale},
year = {2016},
month = {11},
pages = {},
title = {Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models}
}

@inproceedings{liu_structured_2018,
author = {Liu, Hao and He, Lirong and Bai, Haoli and Dai, Bo and Bai, Kun and Xu, Zenglin},
title = {Structured Inference for Recurrent Hidden Semi-Markov Model},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Segmentation and labeling for high dimensional time series is an important yet challenging task in a number of applications, such as behavior understanding and medical diagnosis. Recent advances to model the nonlinear dynamics in such time series data has suggested involving recurrent neural networks into Hidden Markov Models. Despite the success, however, this involvement has caused the inference procedure much more complicated, often leading to intractable inference, especially for the discrete variables of segmentation and labeling. To achieve both flexibility and tractability in modeling nonlinear dynamics of discrete variables and to model both the long-term dependencies and the uncertainty of the segmentation labels, we inherits the Recurrent Hidden Semi-Markov Model and presents an effective bi-directional inference method. In detail, the proposed bi-directional inference network reparameterizes the categorical segmentation with the Gumbel-Softmax approximation and resorts to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2447–2453},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}
@inproceedings{
dai2017recurrent,
title={Recurrent Hidden Semi-Markov Model},
author={Hanjun Dai and Bo Dai and Yan-Ming Zhang and Shuang Li and Le Song},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HJGODLqgx}
}

@article{fine_hierarchical_1998,
	title = {The {Hierarchical} {Hidden} {Markov} {Model}: {Analysis} and {Applications}},
	volume = {32},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007469218079},
	doi = {10.1023/A:1007469218079},
	abstract = {We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek a systematic unsupervised approach to the modeling of such structures. By extending the standard Baum-Welch (forward-backward) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data. We then use the trained model for automatic hierarchical parsing of observation sequences. We describe two applications of our model and its parameter estimation procedure. In the first application we show how to construct hierarchical models of natural English text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can be used to automatically identify repeated strokes that represent combination of letters in cursive handwriting.},
	number = {1},
	journal = {Machine Learning},
	author = {Fine, Shai and Singer, Yoram and Tishby, Naftali},
	month = jul,
	year = {1998},
	pages = {41--62},
}

@article{baum_maximization_1970,
	title = {A {Maximization} {Technique} {Occurring} in the {Statistical} {Analysis} of {Probabilistic} {Functions} of {Markov} {Chains}},
	volume = {41},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2239727},
	number = {1},
	urldate = {2023-05-07},
	journal = {The Annals of Mathematical Statistics},
	author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
	year = {1970},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {164--171},
	file = {JSTOR Full Text PDF:/home/kabalce/Zotero/storage/D52JSL59/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf:application/pdf},
}

@article{baum_inequality_1967,
	title = {An inequality with applications to statistical estimation for probabilistic functions of {Markov} processes and to a model for ecology},
	volume = {73},
	issn = {0273-0979, 1088-9485},
	url = {https://www.ams.org/bull/1967-73-03/S0002-9904-1967-11751-8/},
	doi = {10.1090/S0002-9904-1967-11751-8},
	abstract = {1. Summary. The object of this note is to prove the theorem below and sketch two applications, one to statistical estimation for (proba-bilistic) functions of Markov processes [l] and one to Blakley's model for ecology [4]. 2. Result. THEOREM. Let P(x)=P(\{xij\}) be a polynomial with nonnegative coefficients homogeneous of degree d in its variables \{\#\#\}. Let x= \{\#\#\} be any point of the domain D: \#\# §:(), ]pLi \#\# = 1, i = l, • • • , p, j=l, • • • , q\%. For x= \{xij\} ££{\textgreater} let 3(\#) = 3\{\#\#\} denote the point of D whose i, j coordinate is (dP{\textbackslash} {\textbackslash} f « dP 3(*){\textless}i = (Xij 7—) / 2* *{\textless}i — {\textbackslash} dXij{\textbackslash}(X)// ,-i dXij (»{\textgreater} Then P(3(x)){\textgreater}P(x) unless 3(x)=x. Notation, fi will denote a doubly indexed array of nonnegative integers: fx= \{M\#\vphantom{\{}\}{\textgreater} i = l{\textgreater} • • • {\textgreater} {\textless}lu i=l, • • • , A \#* then denotes Ilf-iHî-i{\textasciicircum}* Similarly, c M is an abbreviation for C[ MiJ \}. The polynomial P(\{xij\}) is then written P(x) = ]CM V{\textasciicircum}-In our notation : (1) 3(\&)*i = (Z) «Wnys*) / JLH CpiiijX».},
	language = {en},
	number = {3},
	urldate = {2023-05-07},
	journal = {Bulletin of the American Mathematical Society},
	author = {Baum, Leonard E. and Eagon, J. A.},
	year = {1967},
	pages = {360--363},
	file = {Full Text:/home/kabalce/Zotero/storage/2KY85KU2/Baum and Eagon - 1967 - An inequality with applications to statistical est.pdf:application/pdf},
}

@unpublished{elbo,
    author = {Willie Neiswanger},
    title = {{Variational} ({Bayesian}) Inference and Mean Field Approximations},
    note={Probabilistic Graphical Models. Lecture 13},
    year = {2017}
}

@inproceedings{nn_ode,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@unpublished{balcerek,
    author = {Balcer, Klaudia and Lipinski, Piotr},
    title = {Extending {DenseHMM} with Continuous Emission},
    note = {ICONIP2023, November 2023}
}

@article{adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@inproceedings{motion_data,
author = {Malekzadeh, Mohammad and Clegg, Richard G. and Cavallaro, Andrea and Haddadi, Hamed},
title = {Mobile Sensor Data Anonymization},
booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
series = {IoTDI '19},
year = {2019},
isbn = {978-1-4503-6283-2},
location = {Montreal, Quebec, Canada},
pages = {49--58},
numpages = {10},
url = {http://doi.acm.org/10.1145/3302505.3310068},
doi = {10.1145/3302505.3310068},
acmid = {3310068},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {adversarial training, deep learning, edge computing, sensor data privacy, time series analysis},
} 

@unpublished{elbo_yt,
    author = {Ghodsi, Ali},
    title = {Variational Autoencoder},
    note={Deep Learning. Lecture 6.2},
    url={https://www.youtube.com/watch?v=uaaqyVS9-rM&t=1182s},
    year = {2017}
}

@book{quasi_random,
	address = {Cham},
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Monte {Carlo} and {Quasi}-{Monte} {Carlo} {Methods}: {MCQMC} 2020, {Oxford}, {United} {Kingdom}, {August} 10–14},
	volume = {387},
	isbn = {978-3-030-98318-5 978-3-030-98319-2},
	shorttitle = {Monte {Carlo} and {Quasi}-{Monte} {Carlo} {Methods}},
	url = {https://link.springer.com/10.1007/978-3-030-98319-2},
	language = {en},
	urldate = {2023-08-30},
	publisher = {Springer International Publishing},
	editor = {Keller, Alexander},
	year = {2022},
	doi = {10.1007/978-3-030-98319-2},
	keywords = {control variates, density estimation, light transport simulation, Low-discrepancy sequences, Markov chain Monte Carlo (MCMC), MCQMC, Monte Carlo, neural networks, quasi-Monte Carlo, quasi-Monte Carlo software, randomized algorithms, sampling, stochastic simulation, variance reduction},
	file = {Submitted Version:/home/kabalce/Zotero/storage/BF3HQCJV/Keller - 2022 - Monte Carlo and Quasi-Monte Carlo Methods MCQMC 2.pdf:application/pdf},
}